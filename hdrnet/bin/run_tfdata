#!/usr/bin/env python3

import argparse
import os
import sys

import tensorflow as tf
import tqdm
import pandas as pd

from hdrnet import models, data_pipeline as dps, utils, metrics


def print_err(message):
    print(message, file=sys.stderr)


def load_params_from_checkpoint(config, checkpoint_path):
    if os.path.isdir(checkpoint_path):
        checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)

    with tf.Session(config=config) as sess:
        if checkpoint_path is None:
            print_err('Could not find a checkpoint in {}'.format(checkpoint_path))
            sys.exit(-1)

        meta_path = ".".join([checkpoint_path, "meta"])
        print('Loading graph from {}'.format(meta_path))
        tf.train.import_meta_graph(meta_path)

        model_params = utils.get_model_params(sess, 'model_params')
        data_params = utils.get_model_params(sess, 'data_params')

        if 'lr_params' in model_params:
            data_params['lr_params'] = model_params['lr_params']

    return checkpoint_path, model_params, data_params


def setup_graph(model_params, inputs):
    model_name = model_params['model_name'].decode()
    if not hasattr(models, model_name):
        print_err("Model {} does not exist".format(model_params.model_name))
        sys.exit(-1)

    mdl = getattr(models, model_name)

    with tf.name_scope('eval'):
        with tf.variable_scope('inference'):
            prediction = mdl.inference(
                inputs['lowres_input'], inputs['image_input'], inputs['params'],
                model_params, is_training=False
            )

            prediction = tf.clip_by_value(prediction, 0, 1)
            loss = metrics.l1_loss(inputs['image_output'], prediction)

            return [
                inputs['image_input'],
                prediction,
                inputs['image_output'],
                loss,
                inputs['input_filename']
            ]


def save_images(input, output, target, filename, args):
    pass


def save_report(df, args):
    pass


def run_evaluation(samples, total_samples, session, args):
    losses = []
    tf.train.start_queue_runners()
    for _ in tqdm.tqdm(range(total_samples)):
        try:
            input, output, target, loss, filename = session.run(samples)

            save_images(input, output, target, filename, args)

            losses.append(dict(name=str(filename), loss=float(loss)))
        except Exception:
            pass
    return pd.DataFrame(losses)


def setup_data_pipeline(name, path, data_params, output_resolution):
    with tf.variable_scope('eval_data'):
        pipeline = getattr(dps, name)(
            path,
            shuffle=False,
            num_epochs=1,
            batch_size=1, nthreads=2,
            fliplr=False, flipud=False, rotate=False,
            random_crop=False,
            params=data_params,
            validate_sizes=False,
            output_resolution=output_resolution
        )
        return pipeline


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        "Runs inference on data using same training data pipeline"
    )

    parser.add_argument(
        "checkpoint_path",
        help="Path to checkpoint file or logdir. If it points to dir, latest checkpoint will be used"
    )
    parser.add_argument(
        "data_path",
        help="Path to data with structure required by specific pipeline"
    )

    parser.add_argument(
        "--data-pipeline",
        help="Data pipeline to use",
        default=dps.__all__[0], choices=dps.__all__
    )
    parser.add_argument(
        "--output-resolution",
        type=int, nargs=2, default=[400, 600]
    )

    parser.add_argument(
        "--output-path",
        help="Path to directory where output will be saved. "
             "If not passed, output will be discarded and only loss will be computed"
    )

    parser.add_argument(
        "--no-target",
        help="Do not save target image to output dir",
        action="store_false", dest="save_target"
    )
    parser.add_argument(
        "--no-input",
        help="Do not save input image to output dir",
        action="store_false", dest="save_input"
    )

    args = parser.parse_args()

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True

    checkpoint_path, model_conf, data_conf = load_params_from_checkpoint(config, args.checkpoint_path)

    data = setup_data_pipeline(args.data_pipeline, args.data_path, data_conf, args.output_resolution)

    outputs = setup_graph(model_conf, data.samples)

    with tf.Session(config=config) as sess:
        result = run_evaluation(outputs, data.nsamples, sess, args)

    print("Mean L1 error: {}".format(result.loss.mean()))


